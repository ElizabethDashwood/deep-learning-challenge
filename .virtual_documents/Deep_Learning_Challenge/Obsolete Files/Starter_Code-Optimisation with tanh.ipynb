


# Import our dependencies
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import tensorflow as tf

#  Import and read the charity_data.csv.
import pandas as pd
application_df = pd.read_csv("https://static.bc-edx.com/data/dla-1-2/m21/lms/starter/charity_data.csv")
application_df.head()


# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.
application_df = application_df.drop(columns=['EIN', 'NAME'])
application_df.head()


# Determine the number of unique values in each column.
# Reference: https://stackoverflow.com/questions/30503321/finding-count-of-distinct-elements-in-dataframe-in-each-column
application_df.nunique()


# Look at APPLICATION_TYPE value counts to identify and replace with "Other"
application_df['APPLICATION_TYPE'].value_counts()


# Choose a cutoff value and create a list of application types to be replaced
# use the variable name `application_types_to_replace`
application_types_to_replace = ["T9", "T13", "T12", "T2", "T25", "T14", "T29", "T15", "T17"]

# Replace in dataframe
for app in application_types_to_replace:
    application_df['APPLICATION_TYPE'] = application_df['APPLICATION_TYPE'].replace(app,"Other")

# Check to make sure replacement was successful
application_df['APPLICATION_TYPE'].value_counts()


# Look at CLASSIFICATION value counts to identify and replace with "Other"
application_df['CLASSIFICATION'].value_counts()


# You may find it helpful to look at CLASSIFICATION value counts >1
# Reference: https://www.geeksforgeeks.org/how-to-display-all-rows-from-dataframe-using-pandas/
# Display all 71 Classifications and their value counts
# pd.set_option('display.max_rows', None)
classification_count_series = application_df['CLASSIFICATION'].value_counts()
classification_count_series



# Choose a cutoff value and create a list of classifications to be replaced
# use the variable name `classifications_to_replace`
classifications_to_replace = ["C7000", "C1700", "C4000", "C5000", "C1270", "C2700", "C2800", "C7100", "C1300",
                              "C1280", "C1230", "C1400", "C7200", "C2300", "C1240", "C8000", "C7120", "C1500", 
                              "C1800", "C6000", "C1250", "C8200", "C1238", "C1278", "C1235", "C1237", "C7210", 
                              "C2400", "C1720", "C4100", "C1257", "C1600", "C1260", "C2710", "C0", "C3200", "C1234",
                              "C1246", "C1267", "C1256", "C2190", "C4200", "C2600", "C5200", "C1370", "C1248", 
                              "C6100", "C1820", "C1900", "C1236", "C3700", "C2570", "C1580", "C1245", "C2500",
                              "C1570", "C1283", "C2380", "C1732", "C1728", "C2170", "C4120", "C8210", "C2561",
                              "C4500", "C2150"]

# Replace in dataframe"
for cls in classifications_to_replace:
    application_df['CLASSIFICATION'] = application_df['CLASSIFICATION'].replace(cls,"Other")

# Check to make sure replacement was successful
application_df['CLASSIFICATION'].value_counts()


# Convert categorical data to numeric with `pd.get_dummies`

# Reference: https://stackoverflow.com/questions/71705240/how-to-convert-boolean-column-to-0-and-1-by-using-pd-get-dummies
# Reference: https://stackoverflow.com/questions/24109779/running-get-dummies-on-several-dataframe-columns
encoded_application_df = pd.get_dummies(application_df, columns=['APPLICATION_TYPE', 'AFFILIATION', 'CLASSIFICATION', 'USE_CASE', 'ORGANIZATION',
                                                 'INCOME_AMT', 'SPECIAL_CONSIDERATIONS'], dtype='int')
encoded_application_df.head()


# Split our preprocessed data into our features and target arrays
y = encoded_application_df.IS_SUCCESSFUL.values
X = encoded_application_df.drop(columns = 'IS_SUCCESSFUL').values

# Split the preprocessed data into a training and testing dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)


# Create a StandardScaler instances
scaler = StandardScaler()

# Fit the StandardScaler
X_scaler = scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)





# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.
nn_model = tf.keras.models.Sequential()

# First hidden layer
nn_model.add(tf.keras.layers.Dense(units=80, activation="relu", input_dim=43))
# Second hidden layer
nn_model.add(tf.keras.layers.Dense(units=30, activation="tanh")) # Trying tanh instead of relu in 2nd hidden layer

# Output layer
nn_model.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

# Check the structure of the model
nn_model.summary()


# Compile the model
nn_model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])



# Train the model
fit_model = nn_model.fit(X_train_scaled, y_train, epochs=100)


# Evaluate the model using the test data
model_loss, model_accuracy = nn_model.evaluate(X_test_scaled,y_test,verbose=2)
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")


# Export our model to HDF5 file
# Reference: https://stackoverflow.com/questions/43402320/export-tensorflow-weights-to-hdf5-file-and-model-to-keras-model-json
# Reference: https://www.tensorflow.org/tutorials/keras/save_and_load
# Reference: https://stackoverflow.com/questions/29831052/error-importing-h5py
import h5py
nn_model.save('AlphabetSoupCharity_Optimisation_tanh_2nd_hidden_layer.h5')



